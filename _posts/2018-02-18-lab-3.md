---
title: "Classifier 1"
header:
  teaser: /assets/images/Lab3.png
categories:
  - Digital Signal Processing
tags:
  - mfcc
  - filter banks
  - rhythm
  - tonality
toc: true
---

# Introduction
In this project we use 150 songs with 6 different genres to compare and try to classify certain tracks as certain genres. This process is similar to what apps like Spotify do to classify a large library of songs into certain genres. For this we use features such as MFCC, filter banks, rhythm, and tonality to create a classifier. 

<figure>
	<a href="/assets/images/Lab3.png"><img src="/assets/images/Lab3.png"></a>
</figure>

# Main Project

## 1. Intro

Lab 3 is the final lab on digital signal processing methods for content-based music information retrieval. In this lab, we will be comparing various distances between audio tracks and using those distances along with various classification methods to automatically detect the genre of a given track. The distances we are using are based on the MFCC coefficients and rhythm that we derived in the previous labs.  
An algorithm being developed will exploit 150 music tracks as training samples to learn the association between genres and songs. The performance of the algorithm will be evaluated using songs with known labels that aren‚Äôt part of the training set.  
The end goal of the lab is to have this algorithm to be fully functional such that if a song is uploaded a label can be returned that contains the genre of the song along with a certain probability. An example of this would be if we provided the song Bach‚Äôs Violin Concerto in A minor No. 1, the algorithm would be able to classify this track as classical with high probability. 

## 2. Implementation of the Distance Matrix 

There are 6 genres of music that need to be classified in this lab. They are: classical, electronic, jazz, punk, rock, and world. These are split into 25 songs for each genre to total the 150 music tracks we are using for the lab. To do this we are using a simple technique looking the k-nearest neighbors. After condensing our previous MFCC from 40 bins to 12 bins, we can accurately compare the MFCC of each track using the MFCC function developed in lab 2. Using the MFCC we calculate the Kullback-Leibler divergence: 

![Equation31]({{ site.url }}/assets/images/equation3-1.PNG)

Finally, the KL distance is rescaled using an exponential kernel and we define the distance between the tracks s and s‚Äô as  

![Equation32]({{ site.url }}/assets/images/equation3-2.PNG)

	Assignment 
	1. Implement the computation of the distance D given 
	by (6). Your function should be able to use the 12 
	merged MFCC coefficients or the Normalized Pitch Class Profile. 
	2. Compute the matrix of pairwise distances for the entire set S 
	
 	D(s,s‚Äô) where s,s‚Äô = 1,...,150 
	
	Display the distance matrix as an image, and discuss its structure. 
	
	3. For each genre, compute the histogram composed of the 
	pairwise distances within that genre. Plot the six 
	histograms on the same figure. Comment about what you see 
	in the figure. 
	
	4. Compute the 6 √ó 6 average distance matrix between the genres, defined by 
	
![Equation33]({{ site.url }}/assets/images/equation3-3.PNG)

	5. Experiment with different values of Œ≥, and find a value of Œ≥ 
	that maximizes the separation between the different genres, as 
	defined by the 6 √ó 6 average distance matrix ùê∑ ÃÖ. 

	
1. The idea behind doing this is to determine how distant or how similar two different songs are.  A gamma of 0.001 is used as with a lower gamma it is easier to see the similarities between tracks. My implementation of the computation of D is at the end of the report in sections A.2.3 and A.2.4. 

2. Code used to compile and compute the matrix D(s, s‚Äô) is below. 

```
close all; 
%clear all; 
clc;   
tic pathName = 'C:\Users\house_000\Documents\MATLAB\lab3folder'; 
fileList = dir(fullfile(pathName, '*.wav')); 
[~, reindex] = sort(str2double(regexp({fileList.name}, '\d+', 'match','once'))); 
fileList = fileList(reindex); 
numSong = size(fileList, 1); 
mfcc1 = cell(150,1); 
fftSize = 512; 
w = kaiser(fftSize); 
fftSizeC = 2048; 
gamma = 0.002;   
%For different lengths of songs, using gamma of 0.001 
parfor i = 1:numSong     
	[song,fs] = audioread(fileList(i).name);     
	ExtractedSong = extractSound(fileList(i).name);     
	mfccCoef = mfcc(ExtractedSong, fs, fftSize, w);     
	mfcc1{i} = cov_mu(mfccCoef); 
end   
mfccDist = zeros(150,150);   
for i = 1:numSong     
	for j = 1:i        
		mfccDist(i,j) = distance(mfcc1{i},mfcc1{j},gamma);         
		mfccDist(j,i) = mfccDist(i,j);     
	end 
end 
h = figure; 
imagesc(mfccDist); 
colormap 'jet'; 
colorbar; 
title({'Distance Matrix: Gamma'; gamma}); 
xlabel('Song 1') ylabel('Song 2'); 
saveas(gca,'distance_matrix001.png'); 
close(h);   
genreList = {'classical','electronic','jazz','metal','rock','world'};   
%3.For each genre, compute the histogram composed of the pairwise  
%distances within that genre. Plot the six histograms on the same figure.    
h = figure; 
hold on 
index = 1; 
for i = 1:25:150     
	%taking upper triangular ensure that commutative comparisons aren't     
	%made     
	binranges = linspace(0,1,100);     
	his1 = triu(mfccDist(i:i+24, i:i+24),1);     
	his2 = nonzeros(his1);     
	plot(binranges, histc(his2, binranges)); 
end 
saveas(gca, 'notAHistogram_mfcc_dist.png'); 
close(h);
%end of 3   
%4. Calculate the average matrix   
distMFCCAvg = zeros(6,6);   
for i = 1:25:150     
	for j = 1:25:150         
		i_ = fix(i/25);         
		j_ = fix(j/25);         
		distMFCCAvg(i_+1, j_+1) = mean(mean(mfccDist(i:i+24, j:j+24)))/625;     
	end 
end 
 
% graphing average distance matrix 
h = figure; 
imagesc(distMFCCAvg); 
ax = gca; 
colormap 'jet'; 
colorbar; 
title({'MFCC average with gamma:'; gamma}); 
xlabel('Genre A'); 
ylabel('Genre B'); 
ax.XTickLabels = {'Classcal', 'Electronic', 'Jazz', 'Punk', 'Rock', 'World'}; 
ax.YTickLabels = {'Classcal', 'Electronic', 'Jazz', 'Punk', 'Rock', 'World'}; 
saveas(gca,'MFCC_average001.png'); 
close(h);   
% End problem 4 
 
% 8. Implement a classifier based on the MFCC coefficients, modified % distance implemented in (6) and genre   
confusionMatrix = zeros(6,6); 
for i = 1:6     
	confusionMatrix(i,:) = classifier(mfccDist, i); 
end 
Accuracy = confusionMatrix/25; 
 
% Make a graph of the confusion matrix 
h=figure; 
confuseString=num2str(confusionMatrix(:)); 
textStrings = strtrim(cellstr(confuseString)); 
imagesc(confusionMatrix./25); 
ax=gca; 
for i=1:6     
	for j=1:6         
		text(j,i,textStrings{sub2ind([6,6],i,j)});     
	end 
end 
colorbar; 
% colormap jet; % can't see numbers with "jet" 
title({'Confusion Matrix'}); 
xlabel('Genre A'); 
ylabel('Genre B'); 
ax.XTickLabels={'Classical','Electronic','Jazz','Punk','Rock','World'}; 
ax.YTickLabels={'Classical','Electronic','Jazz','Punk','Rock','World'}; 
saveas(ax,'confusionMatrix.png'); 
close(h); 
%end of problem 8 
 
%9. Using cross validation as explained in section 5.2, evaluate your  
%classification algorithm. Include in your report the mean and standard  
%deviation matrices you computed from the 50 confusion matrices.   
p = zeros(6,25); 
total = zeros(6,6); 
recordMatrix = zeros(10,6); 
for n = 1:10     
	testSong = zeros(6,5);     
	trainSong = zeros(6,20);     
	for g = 1:6         
		p(g,:) = randperm(25)+25*(g-1);         
		testSong(g,:) = p(g,1:5);     
	end          
	i = 1;     
	indices = zeros(1,30);     
	testingSet = sort(reshape(testSong,[1,30]));     
	mfccDist(testingSet(1,1:30),testingSet(1,1:30)) = 0;     
	confusionCrossMatrix = zeros(6,6);     
	for gen = 1:6         
		confusionCrossMatrix(gen,:) = classifierCross(mfccDist, p(gen, 1:5));     
	end     
	total = confusionCrossMatrix + total;     
	recordMatrix(n,:) = reshape(diag(confusionCrossMatrix),1,6); 
end   
averageMatrix = total/10; 
average = zeros(1,6); 
standMatrix = zeros(1,6);   
for a = 1:6     
	average = mean(recordMatrix)/5;     
	standMatrix = std(recordMatrix); 
end 
 
%9. End of problem 9 
 
%11. Improve the classifier using SVM 
%create an empty cell array 
for i=1:25     
	classname{i} = 'Classical';     
	classname{i+25} = 'Electronic';     
	classname{i+50} = 'Jazz';     
	classname{i+75} = 'Metal';     
	classname{i+100} = 'Rock';     
	classname{i+125} = 'World'; 
end   
Md1 = fitcecoc(mfccDist,classname); 
Md1.ClassNames CodingMat = Md1.CodingMatrix 
 
%A one-versus-one coding design on three classes yields three binary learners. Columns of CodingMat correspond to learners and rows correspond to classes. The class order is the order in Md1.ClassNames,  
 
%Accessing each binary learners using cell indexing and dot notation 
 
for i= 1:6     
	Md1.BinaryLearners{i};     
	Md1.BinaryLearners{i}.SupportVectors 
end 
 
%compute the in-sample classification error 
 
isLoss = resubLoss(Md1) 
%classification error is small, but the classifier might have been overfit. 
%You can cross validate the classifier using |crossval|.   
 
toc 
```

Below is the image of the distance matrix using a gamma of 0.001. There is a lot of red in the top left corner of the image for the first 25 tracks as those are all classical tracks which are very similar to each other, but compared to other tracks classical doesn‚Äôt share too many similarities, other than some jazz and world tracks. Looking at the electronic tracks, many of the tracks are very similar, but some electronic tracks don‚Äôt share as much similarities, and electronic music seems similar to punk and rock music, but not similar to classical jazz and world music. With jazz music it seems that there are a few similarities, but also jazz tracks that are much different compared to other jazz tracks. Jazz music is the most similar to classical music and world music, but pretty different from punk and rock. Meanwhile, punk and rock music seem to go under the same category as the entire region is red and also has some similarities to electronic music. Lastly, world music is all over the place and doesn‚Äôt seem to share too many similarities with any genre. 

![Figure31]({{ site.url }}/assets/images/figure3-1.PNG)

3. The code to implement is histograms is icnluded in the code block above in section 2. 

The histograms below shows the average distance matrix between genres. As it can be seen classical, electronic, jazz, metal, rock all have songs very similar to each other with peaks all around one, while it is difficult to determine songs that are part of the world genre due to the extremely random nature of these songs. This makes sense especially when we look at our distance matrix above.
I‚Äôve also included a ‚Äúhistogram‚Äù that using different color lines.  

![Figure32]({{ site.url }}/assets/images/figure3-2.PNG)
![Figure33]({{ site.url }}/assets/images/figure3-3.PNG)

4. Implementation of the average distance matrix is also above in the code block section 2. Below is the average distance matrix that was calculated. 

![Figure34]({{ site.url }}/assets/images/figure3-4.PNG)

5. While using smaller values of Œ≥ requires more time for the distance matrix to be calculated, it is very easy to see the similarities between tracks once finished. Œ≥ values of 0.001, 0.01, 0.1, 0.3, 0.5, and 0.7 are used for comparison and figures of these values are at the end of the report in section. Looking at the charts it can be seen that using larger values of gamma is a good way to show that a genre is similar to itself which should be obvious, but it is more difficult to see how similar a genre is to another genre. Thus, it can be concluded that if one just wants to compare two very similar songs of the same genre then a high value of gamma should be used, but if one is comparing many different tracks a low gamma should be used. 

## 3. K-Nearest Neighbor Confusion Matrix 

The goal in this part of the lab is that if we are given a certain song, we wish to determine its genre. We will do this by computing the distance between the certain song and every song in a training set and determine the 5 nearest songs to the selected song. Then we see what genre the 5 nearest songs are in and determine the selected song to be that genre. We will generate a confusion matrix from this in order to determine how certain we are that a song belongs to a certain genre.

	Assignment 
	8. Implement a classifier as explained above using the 
	following ingredients:
	‚Ä¢ The 12 mfcc coefficients or the 12 Normalized Pitch Class Profile 
	‚Ä¢ The modified Kullback-Leibler distance d defined by (6) 
	‚Ä¢ The majority vote based on the 5 nearest neighbors 
	9. Using cross validation as explained in section 5.2, 
	evaluate your classification algorithm. Include in your 
	report the mean and standard deviation matrices you 
	computed from the 50 confusion matrices. 
	10. Compare the performance of the classification using 
	the 12 mfcc coefficients to that of using the 12 
	Normalized Pitch Class Profile. 
	
8. Implementation of a classifier is included below. 

```
function[index] = findNMinimum(input,n)     
sortMatrix = sort(input);     
len = size(input,2);     
minValue = sortMatrix(len-4:len);     
index = zeros(1,n);     
for i = 1:n         
	index(1,i) = find(input == minValue(1,i));     
end 
end
```

```
function [output] = classifier(distance, rows)     
%classifier to compare a song to other 149 songs     
n = 5;     
index = zeros(25,n);     
temp = zeros(1,25);     
row = rows*25-24;     
output = zeros(1,6);     
loopi = 1;  
for i = row:row+24         
	index(loopi,:) = findNMinimum(distance(i,:), 5);         
	loopi = loopi + 1;     
end     
index = floor(index/25) + 1;       
for j = 1:25         
	temp(1,j) = mode(index(j,:));     
end       
for k = 1:25         
	switch(temp(1,k))             
		case 1                 
			output(1,1) = output(1,1) + 1;             
		case 2                 
			output(1,2) = output(1,2) + 1;             
		case 3                  
			output(1,3) = output(1,3) + 1;            
		case 4                 
			output(1,4) = output(1,4) + 1;             
		case 5                 
			output(1,5) = output(1,5) + 1;             
		case 6                 
			output(1,6) = output(1,6) + 1;         
	end     
end 
end 
```

Below is a figure of the confusion matrix, it can be seen that classical is classified perfectly, while electronic and jazz and easy to detect as well. Punk and rock have a bit less certainty, while world has less than a 50% chance of being identified and has similarities to classical as well which makes sense from the analysis of the distance matrix. The numbers aren‚Äôt the same as what the professor provided, but comparing to the similarity matrix this result is expected.  

![Figure35]({{ site.url }}/assets/images/figure3-5.PNG)

9. The implementation of the cross validation is included below:

```
function[output] = classifierCross(distance, distIndex)    
%cross validation classifier to use a sample set of songs to test 
n = 5; 
index = zeros(25,n); 
temp = zeros(1,25); 
output = zeros(1,6);   
loopi = 1;   
for i = 1:5     
	index(loopi,:) = findNMinimum(distance(distIndex(1,i),:),5);     
	loopi = loopi + 1; 
end   
index = floor(index/25) + 1; 
for j = 1:5     
	temp(1,j) = mode(index(j,:)); 
end 
for k = 1:5     
	switch(temp(1,k))         
		case 1             
			output(1,1) = output(1,1) + 1;         
		case 2             
			output(1,2) = output(1,2) + 1;         
		case 3             
			output(1,3) = output(1,3) + 1;         
		case 4             
			output(1,4) = output(1,4) + 1;         
		case 5             
			output(1,5) = output(1,5) + 1;         
		case 6             
			output(1,6) = output(1,6) + 1;      
	end 
end 
end
```

There are two mean matrices and the standard deviation matrices shown below for the cross validation, from this it can be seen that classical is being identified the best, followed by jazz, punk, and electronic. World and rock are difficult to classify. This can be seen in both matrices.  

![Figure36]({{ site.url }}/assets/images/figure3-6.PNG)

## 4. Improve the classifier 

You can improve the classification algorithm in several ways. 

1. Use temporal, or rhythm features to compute a distance that does not depend solely on the distribution of notes. In other words, incorporate melody into the distance measure. 

2. Use a more sophisticated classification algorithm than k-nearest neighbors to classify test set tracks. Examples include: linear or quadratic discriminant analysis, support vector machine, and trees. Look at the documentation of the corresponding functions in the MATLAB statistics and bioinformatics toolboxes, and use the appropriate MATLAB function to implement such a classifier. Evaluate its performance using cross-validation (see section 5.2) and compare it to the k-nearest neighbor classifier. 
 
	Assignment 
	11. Improve the classifier using one of the suggestions above. 
	
Code used to improve the classifier using a support vector machine is included in the code block for the distance matrix above. I am calculating the in-sample classification error in my code to see if the support vector machine helps. The errors I got were 0.0467, 0.0333, 0.0467, 0.0467, 0.0267, and 0.0533. The error is pretty small, so it is unclear whether the SVM worked or not, but it does seem to improve the classifier to a certain extent. 

## 5. Conclusion

Through this lab, we were able to classify what genre a certain track is by calculating the distance between tracks and finding k-nearest neighbors of songs to a given song to create a confusion matrix determining a genre of a song. We then implemented an SVM to improve the classifier, it was difficult to determine whether or not the SVM helped as the sample set was too small for it to make a difference. The goal of the lab was reached as we were able to accurately determine the genre of a song, except for world which was to be expected as it is a catch all genre.  
 








 




