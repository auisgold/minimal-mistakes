---
title: "Rhythm and Chroma"
header:
  teaser: /assets/images/Lab2.png
categories:
  - Digital Signal Processing
tags:
  - rhythm
  - chroma
toc: true
---

# Introduction
In this project we look at how similar certain seconds in a music track are to each other, as well as look into the rhythm and chroma of a certain music track. We will use these features in a future project to compare two music tracks to see if they are part of the same genre of music using this analysis. 
<figure>
	<a href="/assets/images/Lab2.png"><img src="/assets/images/Lab2.png"></a>
</figure>

# Main Project

## 1. Intro

In this lab, we look at more digital signal processing for content-based music information retrieval. The two main features we are looking at are rhythm and tonality. We will be using the same tracks for lab 1 to investigate these features.  
These features are more useful for all frequencies of music as opposed to the features investigated in lab one that only helped for low frequency music. 

## 2. Rhythm 

Musical rhythm can be loosely defined as the presence of repetitive patterns in the temporal structure of music. We will implement several techniques to detect the presence of rhythmic structures in an audio track. 
Given an audio segment of duration T = 24 seconds, the goal is to compute a score that measures how often spectral patterns are repeated. Formally, we will compute a vector, B, such that B(lag) quantifies the presence of similar spectral patterns for frames that are lag frames apart. The lag associated with the largest entry in the array B is a good candidate for the period in the rhythmic structure.  
We explain below two approaches to compute the array B. 

### 2.1 Spectral Decomposition

We’ll first compute the mfcc coefficients for a frame as explained in lab 1, thereby obtaining a vector, and then modify each coefficient of the mfcc vector with a nonlinearity. The purpose of the non-linearity is to account for the non-linear perception of loudness by the human auditory system: loudness does not increase linearly with sound intensity (measured, for instance, in Watts/m<sup>2</sup>). A good approximation to the perceptual response of the human auditory system within a given critical band of hearing is obtained by taking the logarithm of the energy in that band. We therefore define an mfcc vector in decibels for each frame as 

![Equation21]({{ site.url }}/assets/images/equation2-1.PNG)

where the logarithm is taken of each element. 

Later you will write a function that returns an mfcc array, mfcc(k,n), comprising the mfcc vectors corresponding to each frame of an audio track. This array will have dimensions

![Equation22]({{ site.url }}/assets/images/equation2-2.PNG)

where N<sub>f</sub> = f<sub>s</sub>T/N. Here N<sub>f</sub> is the number of frames, N = 512 is the frame size in samples, f<sub>s</sub> is the audio sampling rate, and K = N/2 + 1 is the number of frequencies. As in lab 1, we choose nbanks = 40 as the number of mels. The entire analysis described below is performed at the level of frames. 

### 2.2 Spectrum histogram

We propose to construct a visual representation of a musical track by counting how often a given note is played at a given amplitude (we consider each index of the mfcc vector to be a “note”, and measure amplitude in dB). 

	Assignment
	1. Modify your function that computes the mfcc to return 
	the mfcc in decibels. 
	2. Construct a two-dimensional 
	spectrum histogram (a matrix of counts), with 40 
	columns —one for each mfcc index, and 81 rows —one for 
	each amplitude level, measured in dB. The amplitude range 
	we care about is from -20 dB to 60 dB, so clip the mel 
	values to that range. In other words, values less than 
	-20 dB map to -20, values greater than 60 dB map to 60, 
	and values inbetween map to the nearest integer in the 
	set {−20,−19,−18,...,60}. Once you have the counts, 
	normalize the histogram such that the sum along each 
	column (i.e. for each “note”) is one. 
	3. Display, using the MATLAB function imagesc, the 
	spectrum histogram for the 12 audio tracks supplied 
	for the first lab. 
	
1. Here is the MATLAB code of the function mfcc returned in decibels

```
function [ mfccp ] = mfcc( fbank,Xn,~) 
%mfcc Computes the Mel frequency coeffecients 
%   The mel-spectrum (MFCC) coefficient of the n-th frame is defined for 
%   p = 1,...,NB 
narginchk(2, 3); 
% Xn=freqDist(filename); 
mfccp=(fbank*abs(Xn)).^2; % Lets us still use non-log mfcc (if needed) 
if(nargin==3)     
	mfccp=10*log10(mfccp);     
	return; 
end 
end 

function [ soundExtract,p ] = extractSound( filename, time) 
%extractSound Extracts time (in seconds) from the middle of the song 
%   Write a MATLAB function that extract T seconds of music from a 
%   given track. You will use the MATLAB function audioread to 
%   read a track and the function play to listen to the track. 
narginchk(1, 2); 
if(nargin == 1)     
	time = 24; 
end 
info = audioinfo(filename); 
song=audioread(filename); 
if time >= info.Duration     
	soundExtract=song;     
	if(nargout == 2)         
		p=audioplayer(soundExtract,info.SampleRate);     
	end     
	return; 
elseif time<= 1/info.SampleRate     
	error('Too small of a time to sample'); 
end 
samples=time*info.SampleRate;   
soundExtract=song(floor(info.TotalSamples/2)-floor(samples/2):1: ...     
	floor(info.TotalSamples/2)+floor(samples/2)); 
if(nargout == 2)     
	p=audioplayer(soundExtract,info.SampleRate); 
end 
end
```

2, 3. In order to visualize the mfcc, a spectrum histogram is created which maps each mfcc index and the corresponding amplitude levels to a color displaying how often each mfcc index occured at the respective volume. Below is the histogram for ‘track201-classical’ which aside from the very low sounds, most of the pitches were at around the same volume.

![Figure21]({{ site.url }}/assets/images/figure2-1.PNG)

### 2.3 Similarity Matrix

The first stage involves computing a similarity matrix defined as follows 

![Equation23]({{ site.url }}/assets/images/equation2-3.PNG)

Here we are considering the mel spectral signature of each frame to be a vector, and using the dot product to compute S(i,j), the cosine of the angle between the vectors corresponding to frames i and j. For this exercise we use the non-dB version of the mfcc coefficients (i.e., we don’t first take 10 times the base 10 log of the mfcc values). We note that mfcc is always positive in this case, and therefore 0 ≤ S(i,j) ≤ 1. 

The similarity S(i,j) is large, S(i,j) ≈ 1, if the notes being played in frame i are similar to the notes being played in frame j. We note that this similarity is independent of the loudness of the music (the cosine between two vectors does not depend on their magnitude). 

	Assignment 
	4. Implement the computation of the similarity matrix, and display 
	the similarity matrices (color-coded) for the 12 audio tracks 
	supplied for the first lab. Use colormap(’jet’) and imagesc( ). 
	
The code for the similarity matrix and figure of ‘track201-classical’ for the similarity matrix is shown below.

```
function [ sim ] = simMatrix(mfccp,filename,~) 
%simMatrix displays the similarity matrix for the mel coefficients 
%   The similarity matrix is defined as follows: 
%   S(i,j) = sum_{k=1}^{nbanks}{ 
%   (mfcc(k,i)*mfcc(k,j))/(norm(mfcc(:,i)) * norm(mfcc(:,j))) 
% fbank=melBank(); 
% mfccp=mfcc(fbank,filename); 
[a,b]=size(mfccp); % Preallocate matrix 
sim=zeros(b,b); 
normI=sqrt(sum(abs(mfccp).^2,1)); 
parfor i=1:b %frame i     
	for j=1:b %frame j         
		for k=1:a %fbank k             
			sim(i,j)=sim(i,j)+...                 
				(mfccp(k,i)*mfccp(k,j)/(normI(j).*normI(i)));          
		end     
	end 
end   
if(nargin == 3)     
	h=figure;     
	imagesc(sim);     
	xlabel('Frame Number');     
	ylabel('Frame Number');     
	title({'Similarity Matrix:'; filename});     
	colorbar;     
	colormap 'jet';     
	saveas(gca,['SimMatrix' filename(6:end-4) '.png']);     
	close(h); 
end 
end	
```

In the figure it can be seen that classical music is mostly green and blue which is an indication that the music has a lot of different notes being played. As can be heard in the music, classical music has a widespread amount of notes being played throughout, so this physically makes sense for this track. 

![Figure22]({{ site.url }}/assets/images/figure2-2.PNG)

### 2.4 A first estimate of the rhythm 

We note that the entries S(n,n + l) in the similarity matrix (3) are always at a distance (time lag) of l frames. In order to detect the presence of patterns that are repeated every l frames we compute, 

![Equation24]({{ site.url }}/assets/images/equation2-4.PNG)

B(l) is the sum of all the entries on the l<sup>th</sup> upper diagonal. Indeed, B(0) is the sum of all the entries on the main diagonal. Similarly, B(1) is the sum of the entries on the first upper diagonal, etc. We note that 

![Equation25]({{ site.url }}/assets/images/equation2-5.PNG)

The index l ≥ 1 associated with the largest value of B(l) corresponds to the presence of a rhythmic period of l × N/f<sub>s</sub> seconds.

        Assignment
        5. Implement the computation of the rhythm index B(l) defined 
	in (4). Plot the vector B as a function of the lag l = 0,
	Nf −1 for the 12 tracks. Comment on the presence, or absence, 
	of a strong rhythmic pattern. 

The code for the computation of the rhythm index and plot of the rhythm index of ‘track396’ is shown below. 

```
function [ B ] = rhythmIndex( filename,sim,~) 
%rhythmIndex Identifies rhythmic periods 
%   B(l) is the sum of all the entries on the lth upper diagonal. 
%   The index l associated with the largest value of B(l) corresponds to 
%   the presence of a rhythmic period of l*K/f_s seconds
% sim = simMatrix( filename);
[len,~]= size(sim);
B=zeros(1,len);
parfor l=0:len-1
        B(l+1)=(1/(len-l))*sum(diag(sim,l));
end
if(nargin==3)
        h=figure;
        plot(B);
        xlim([0,len]);
        title({'Rhythm Index B(l):'; filename});
        xlabel('lag (frames)');
        ylabel('Presence of rhythmic period');
        saveas(gca,['RhythmIndex' filename(6:end-4) '.png']);
        close(h);
end
end
```

![Figure23]({{ site.url }}/assets/images/figure2-3.PNG)

### 2.5 Periodicity Histograms

Many entries in the similarity matrix are small, and are not useful to quantify the presence of rhythm. Furthermore, it would be interesting to be able to quantify the rhythm as a function of tempo. We can define a notion of tempo, measured in beats per minute, by taking the inverse of the rhythmic period, l · N/f<sub>s</sub>. We get 

![Equation26]({{ site.url }}/assets/images/equation2-6.PNG)

Note the multiplication by 60 to convert beats-per-second to BPM. Re-arranging this formula we can find the l (frame lag) corresponding to a given BPM of interest as 

![Equation27]({{ site.url }}/assets/images/equation2-7.PNG)

Now we’ll describe an algorithm to compute the probability of finding a large similarity at a given tempo of interest. 

	Assignment 
	6. Compute the median 𝑆 of the matrix S. 
	
6. The code to compute the median of S is below:

```
function [ AC ] = autoC( filename,sim,~ ) 
%autoC Computes the autocorrelation of a song 
%   In general, if two frames i and j are similar, we can find out 
%   if they are repeated later in the segment, at time j+l   
info = audioinfo(filename); 
% sim = simMatrix( filename); 
[len,~]=size(sim); 
AC=zeros(1,len);   
parfor l=0:len-1     
	for i=1:len         
		for j=1:len-l            
			AC(l+1)=AC(l+1)+(sim(i,j)*sim(i,j+l));
		end     
	end     
	AC(l+1)=AC(l+1)*(1/(len*(len-l))); 
end   
if(nargin==3)     
	h=figure;     
	xAxis=linspace(0,len/info.SampleRate,len);     
	plot(xAxis,AC);     
	xlim([0,len/info.SampleRate]);     
	title({'Autocorrelation AC(l):'; filename});     
	xlabel('Lag (secs)');     
	ylabel('Autocorrelation');     
	saveas(gca,['AutoC' filename(6:end-4) '.png']);     
	close(h); 
end   
end 
```

### 2.6 A better estimate of the rhythm 

Above we looked at the similarity between frames that are a fixed lag, l, apart in time by computing the vector B(l). However, it is possible to look for patterns within the similarity matrix related to l in other ways. In particular, we can ask the following question: if the spectral signature at time i is related to the signature at time j (i.e. S(i,j) is relatively large), is it also related to the spectral signature at time j + l (i.e. is S(i,j + l) also relatively large?). A lag, l, will be a candidate for a rhythmic period if there are many i and j combinations such that if S(i,j) is large, S(i,j + l) is also large. This leads us to the autocorrelation. 

First, consider a single row, i, in the similarity matrix and the autocorrelation function for that row 

![Equation28]({{ site.url }}/assets/images/equation2-8.PNG)

For a fixed value of i, this summation will tend to be large at values of l which cause peaks in row i to line up with peaks in the same row translated by the amount l. In other words, this summation will be large at values of l corresponding to a periodicity in row i of the S matrix. 
We can get a global measure by averaging these values together over all starting times i 

![Equation29]({{ site.url }}/assets/images/equation2-9.PNG)

	Assignment 
	9. Implement the computation of the rhythm index AC(l) defined 
	in (9). Plot the vector AC as a function of the 
	lag l = 0,1,...,Nf − 1 for the 12 tracks. Comment on the 
	presence, or absence, of a strong rhythmic pattern. 
	See Fig. 2 for an example of a plot of the rhythm index. 
	
MATLAB code for implementation of AC(l) is below:

```
function [ AC ] = autoC( filename,sim,~ ) 
%autoC Computes the autocorrelation of a song 
%   In general, if two frames i and j are similar, we can find out 
%   if they are repeated later in the segment, at time j+l   
info = audioinfo(filename); 
% sim = simMatrix( filename); 
[len,~]=size(sim); AC=zeros(1,len);   
parfor l=0:len-1     
	for i=1:len         
		for j=1:len-l             
			AC(l+1)=AC(l+1)+(sim(i,j)*sim(i,j+l));         
		end     
	end     
	AC(l+1)=AC(l+1)*(1/(len*(len-l))); 
end   
if(nargin==3)     
	h=figure;     
	xAxis=linspace(0,len/info.SampleRate,len);     
	plot(xAxis,AC);     
	xlim([0,len/info.SampleRate]);     
	title({'Autocorrelation AC(l):'; filename});     
	xlabel('Lag (secs)');     
	ylabel('Autocorrelation');     
	saveas(gca,['AutoC' filename(6:end-4) '.png']);     
	close(h); 
end   
end
```

The plot for AC(l) of ‘track396-electronic’ is shown in the figure below. The plot is very similar to the plot of B(l) and the better decision comes down to which method is faster for this specific track. 

![Figure24]({{ site.url }}/assets/images/figure2-4.PNG)

### 2.7 Rhythmic variations over time 

Here we are interested in studying the dynamic changes in the rhythm. For this purpose, we consider short time windows formed by 20 frames (approximately 1 second) over which we compute a vector AC of size 20 

![Equation210]({{ site.url }}/assets/images/equation2-10.PNG)
for l = 0,...,4, and m = 0,...,N<sub>f</sub>/20 − 1. 

	Assignment 
	10. Implement the computation of the rhythm index AC(l,m) defined 
	in (11). Plot the image AC in false color as a function of the 
	lag l = 0,...,19 (y-axis) and the time window index m (x-axis) 
	for the 12 tracks. Comment on the variation of rhythm patterns 
	for the different tracks. 
	
 MATLAB code used to implement AC(l, m) is below:
 
 ```
 function [ AClm ] = rhythmVar( filename,sim,~ ) 
 %Rhythm rhythmVar  
 %   In general, if two frames i and j are similar, we can find out 
 %   if they are repeated later in the segment, at time j+l 
 %   For the sake of this lab, we will be processing 20 frames (~1 second)   
 [len,~]=size(sim); 
 prod=zeros(20,20);   
 for l = 1:20          
 	for m = 1:floor(len/20)-1              
		for i = 1:20                  
			for j = 1:20-l+1                      
				prod(i,j) = sim(i+m*20,j+m*20)*sim(i+m*20,j+m*20+l-1);                 
			end              
		end              
		AClm(m,l) = 1/(20*(20-l))*sum(sum(prod(:,:)));             
		prod = 0;          
	end 
end           
if(nargin==3)     
	h=figure;     
	imagesc(AClm);     
	xlim([0, 20]);     
	ylim([0.5, 1.5])     
	title({'Autocorrelation AC(l,m):'; filename});     
	xlabel('Time (secs)');  
	ylabel('Lag (secs)');     
	colormap 'jet';     
	colorbar;     
	saveas(gca,['RhythmVar' filename(6:end-4) '.png']);     
	close(h); 
end 
end 
```

The following is the rhythmic variation of ‘track396-electronic’ and it can be seen that there are strong rhythmic patterns, but they are mostly the same so the chart is mostly blue.

![Figure25]({{ site.url }}/assets/images/figure2-5.PNG)

## 3. Tonality and Chroma 

### 3.1 The equal-tempered scale 

We now investigate the time-frequency structure related to the concept of tonality and chroma. The tonality is concerned with the distribution of notes at a given time. 
In contrast, the melody characterizes the variations of the spectrum as a function of time. 
We first observe that the auditory sensation that is closely related to frequency, pitch, corresponds to a logarithmic scale of the physical frequency. We have seen examples of such scales in the previous lab: the bark and the mel scales. 
In this lab, we define another logarithmic scale, known as the tempered scale. First, we define a reference frequency f<sub>0</sub> associated with a reference pitch. While it is customary to use the frequency 440 Hz associated with the pitch A4, we will use f<sub>0</sub> = 27.5Hz (this known as A0, the lowest note on a piano). The tempered scale introduces 12 frequency intervals—known as semi-tones—between this reference frequency f<sub>0</sub> and the next frequency also perceived as an A, 2f<sub>0</sub>. As a result, a frequency f<sub>p</sub> that is p semitones away from f<sub>0</sub> is given by 

![Equation211]({{ site.url }}/assets/images/equation2-11.PNG)

An interval of 12 semitones, [f<sub>p</sub>,f<sub>p+12</sub>), corresponds to an octave. The same notes (e.g. A, or C#) are always separated by an octave. For instance, the two previous A notes, A0 and A4, are separated by 4 octaves, since 440 = 24 × 27.5. The notion of note can be formally modeled by the concept of chroma.

### 3.2 Chroma

The chroma is associated with the relative position of a note inside an octave. It is a relative measurement that is independent of the absolute pitch. We describe an algorithm to compute a chroma feature vector for a frame n. To simplify the notation, we drop the dependency on the frame index, n, in this discussion. 

We first present the idea informally, and then we make our statement precise. We are interested in mapping a given frequency, f, to a note. Given a reference frequency f<sub>0</sub>, we can use the following equation to compute the distance p between f and f<sub>0</sub>, in semitones  

p = round(12log<sub>2</sub>(f/f<sub>0</sub>)) 

We note that f is usually not exactly equal to f<sub>0</sub>2<sup>p/12</sup> for an integer value of p, which is why we round 12log<sub>0</sub>(f/f<sub>0</sub>) to the closest integer. 

We can then map this index p to a note, or “chroma”, c, within an octave, using 

c = p mod(12)

or equivalently, 

p = 12q + c, where 0 ≤ c ≤ 11, and q ∈ {0,1,2,...} 

In other words, f<sub>p</sub> and f<sub>p+12</sub> given by (12) correspond to the same note c, which is exactly what you see if you look at the keys of a piano. 

The problem with this approach is that we do not have an algorithm to extract the frequencies f of the notes that are being played at a given time. Rather, the Fourier Transform of a frame gives a distribution of the spectral energy at that frame. We need to identify the main peaks in the Fourier transform, and compute the chroma associated with these frequencies, using the equations above. 

In the following we describe in detail the different steps of an appropriate algorithm. 

#### 3.2.1 Step 1: spectral analysis and peak detection 

The goal is to identify the main notes being played, and to remove the noise and the harmonics coming from the timbre of each instrument. For each frame n, we compute the windowed FFT as explained in the first lab. We obtain K = N/2 + 1 (where N is even) Fourier coefficients, given by 

Y = FFT(w. ∗ x<sub>n</sub>); 
K = N/2 + 1; 
K<sub>n</sub> = N/2 + 1; 

We detect the local maxima of the magnitude of the Fourier transform |X<sub>n</sub>|, and count the number of maxima, or peaks, n<sub>peaks</sub>. Each peak occurs at a particular coefficient value, k, and we let the set P be the set of k’s where peaks occur. We denote by f<sub>k</sub>, where k ∈ P, the corresponding peak frequencies. The matlab function findpeaks( ) 

#### 3.2.2 Step 2: Assignment of the peak frequencies to nearby semitones

Using 27.5 Hz as our value for f<sub>0</sub>, for each peak frequency, f<sub>k</sub>, we find the two semitones nearest to f<sub>k</sub>, p<sub>L</sub>(k) and p<sub>H</sub>(k), via

p<sub>L</sub>(k) = floor(12log<sub>2</sub>(f<sub>k</sub>/f<sub>0</sub>))

and 

p<sub>H</sub>(k) = ceil(12log<sub>2</sub>(f<sub>k</sub>/f<sub>0</sub>))

We then map the index p<sub>L</sub>(k) to the note c<sub>L</sub>(k) via

c<sub>L</sub>(k) = p<sub>L</sub>(k) mod(12) 
and do the same for p<sub>H</sub>(k) to get c<sub>H</sub>(k). 

#### 3.2.3 Step 3: the Pitch Class Profile: a weighted sum of the semitones 

Instead of completely assigning each peak frequency, f<sub>k</sub>, to a single semitone, it is beneficial to spread the effect of f<sub>k</sub> to the two neighboring semitones c<sub>L</sub>(k) and c<sub>H</sub>(k) using a raised cosine function. 

We define the Pitch Class Profile associated with the note, c, as a weighted sum of all the peak frequencies whose ceilings or floors map to c, irrespective of the octave they fall in (recall that c = 0,...,11). 

More precisely, each peak k ∈ P contributes the following amounts to PCP(c<sub>L</sub>(k)) and PCP(c<sub>H</sub>(k)) (assume the PCP variables are initialized to zero) 

PCP(c<sub>L</sub>(k)) = PCP(c<sub>L<sub?(k)) + w<sub>L</sub>|X<sub>n</sub>(k)|<sup>2</sup>

and 

PCP(c<sub>H</sub>(k)) = PCP(c<sub>H</sub>(k)) + w<sub>H</sub>|X<sub>n</sub>(k)|<sup>2</sup>

where the weights w<sub>L</sub> and w<sub>R</sub> are computed via 

r<sub>L</sub> = 12log<sub>2</sub>(f<sub>k</sub>/f<sub>0</sub>) − p<sub>L</sub>(k)
w<sub>L</sub> = cos<sup>2</sup>(πr<sub>L</sub>/2)

and 

r<sub>H</sub> = p<sub>H</sub>(k) − 12log<sub>2</sub>(f<sub>k</sub>/f<sub>0</sub>)

In plain English, PCP(c) tells us how much energy is associated with frequencies that are near to the note c across all the octaves. 

#### 3.2.4 Step 4: Normalize the Pitch Class Profile 

We want our definition of chroma to be independent of the loudness of the music. We therefore need to normalize each PCP by the total PCP computed over all the semitones. The normalized pitch class profile (NPCP) is defined by 

![Equation212]({{ site.url }}/assets/images/equation2-12.PNG)

	Assignment 
	11. Implement the computation of the Normalized Pitch Class Profile, 
	defined by (26). You will compute a vector of 12 entries for each 
	frame n. 
	12. Evaluate and plot the NPCP for the 12 audio tracks supplied 
	for the firstlab: http://ecee.colorado.edu/~fmeyer/class/ecen4532/audio-files.zip 
	See Fig. 4 for an example. 
	
11. MATLAB code used to implement the NPCP is below:

```
function [PCP] = normPCP( filename,x,~) 
%rhythmIndex Identifies rhythmic periods 
%   B(l) is the sum of all the entries on the lth upper diagonal. 
%   The index l associated with the largest value of B(l) corresponds to 
%   the presence of a rhythmic period of l*K/f_s seconds 
info=audioinfo(filename); 
% x=extractSound(filename); 
[s,~,~]=spectrogram(x,kaiser(512),256,512,info.SampleRate,'yaxis'); 
s=abs(s); 
[a,b]=size(s);   
locs=zeros(a,b); 
peaks=zeros(a,b); 
for i=1:b     
	len=length(findpeaks(s(:,i)));     
	[peaks(1:len,i),locs(1:len,i)]=findpeaks(s(:,i)); 
end 
freqVals=(5512/257)*locs; 
f0=27.5; 
sm=round(12*log2(freqVals/f0)); 
r=12*log2(freqVals/f0)-sm; c=mod(sm,12); 
[~,B]=size(c); % 257,1032   
w=(cos(pi*r/2)).^2; % Make pretty with matrix 
w(isnan(w))=0;   
% k is the number of peaks 
% c is the chroma (A,A#,B,etc.) 
% n is the frame number % Weighting function, w, needs to be k*n*c large   
PCP=zeros(12,B);    
% Pick out semitones 
% Loop through each value of c (1,2,...,12)
% Find the corresponding rows to extract from w where semitone is
% (1,2,...,12) multiply by Xn at that row at the corresponding row 
peaks=peaks.^2;   
for i=1:B %nFrames     
	for j=0:11         
		a=find(c(:,i)==j);         
		PCP(12-j,i)=sum(w(a,i).*peaks(a,i));     
	end 
end 
PCP=bsxfun(@rdivide,PCP,sum(PCP));  
PCP = 10*log10(PCP/(max(max(x))));   
if(nargin==3)     
	h=figure;     
	imagesc(PCP);     
	ax=gca;     
	ax.YTickLabel=fliplr({'A ','A#','B ','C ','C#','D ','D#','E ',...         'F ','F#','G ','G#'});    
	ax.YTick=linspace(1,12,12);     
	colormap 'jet';     
	title({'Chroma :'; filename});     
	xlabel('frames');     
	ylabel('Note');     
	colorbar;     
	saveas(gca,['NPCP' filename(6:end-4) '.png']);     
	close(h); 
end 
end
```

12. This is the NPCP for ‘track201-classical’. The graph shows that A is the most common note. It is also noted that this track has multiple instruments playing different notes, which makes sense looking at this graph with the representation of many notes being played simultaneously. 

![Figure26]({{ site.url }}/assets/images/figure2-6.PNG)

## 4. Conclusion

In this lab, we investigated the spectral decomposition, similarity matrices, rhythm, and chroma using the tracks for lab 1. The similarity matrices were able to show us at what points in a song where similar notes were being played and could be used to organize music by the variety of a track of music. The rhythm of the music was helpful for tracks that had a very periodic pattern in the background. The chroma was able to define the different notes in a track in 12 different semitones and analyze what the main note being played is at different times. Problems 7 and 8 were skipped as I ran out of time to implement the functions specified in those two problems. These features analyzed in this lab seem to be more useful than the features investigated in lab 1 as these can be applied to high frequency tracks, rather than just low frequency tracks. 

	





