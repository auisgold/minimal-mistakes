---
title: "Time Domain Analysis: Windowing and Spectral Features"
header:
  teaser: /assets/images/figure1-1.PNG
categories:
  - Digital Signal Processing
tags:
  - time domain
toc: true
---

# Introduction
This is a continuation of time domain analysis, this time looking at windowing and spectral features. We also look into psychoachousic characteristics and create a filter bank for future labs to better determine features of different genres of music. 

<figure>
	<a href="/assets/images/figure1-1.PNG"><img src="/assets/images/figure1-1.PNG"></a>
</figure>

## Main Project

1. **Low level features: time domain analysis**

Music is made up of notes of different pitch. It is only natural that most of the automated analysis of music should be performed in the spectral (frequency) domain. 

	1. Windowing and spectral leaking 
	Our goal is the reconstruction of a musical score. 
	Our analysis requires N = 512 samples to compute notes 
	over a frame. The spectral analysis proceeds as follows. 
	Each frame is smoothly extracted by multiplying the 
	original audio signal by a tapper window w. The Fourier 
	transform of the windowed signal is then computed. 
	If xn denotes a frame of size N = 512 extracted at frame 
	n, and w is a window of size N, then the Fourier transform, 
	Xn (of size N) for the frame n is given by 
	
![Equation11]({{ site.url }}/assets/images/equation1-1.PNG)

	There are several simple descriptors that can be used to 
	characterize the spectral information provided by the 
	Fourier transform over a frame. 
	
	2. 
	Assignment: Let  
	ùë•[ùëõ] = ùëêùëúùë†(ùúî0ùëõ), ùëõ ‚àà ùëç (4)  
	Derive the theoretical expression of the discrete time 
	Fourier transform of x, given by  
	
![Equation12]({{ site.url }}/assets/images/equation1-2.PNG)

	The Fourier transform of the cosine function is easy to 
	compute. It is the sum of 2 exponentials, and the Fourier 
	transform of this is simply the Dirac delta function 
	offset by frequency œâ. Thus the Fourier transform of (4) is: 
	
![Equation13]({{ site.url }}/assets/images/equation1-3.PNG)

	3. 
	Assignment: In practice, we work with a finite signal, 
	and we multiply the signal x[n] by a window w[n]. We assume 
	that the window w is non zero at times n = ‚àíN/2, . . . , N/2, 
	and we define  
	ùë¶[ùëõ] = ùë•[ùëõ]ùë§[ùëõ + ùëÅ/2]  
	Derive the expression of the Fourier transform of y[n] in terms 
	of the Fourier transform of x and the Fourier transform of the 
	window w.
	
	Using knowledge from the lecture class, we know that the Fourier 
	transform of two functions in the time domain is the same as the 
	convolution of two functions in the Fourier domain. We can also 
	use the knowledge that a time shift in the time domain equates 
	to a scaling in the Fourier domain. Using these two properties 
	the Fourier transform of y[n] is: 
	
![Equation14]({{ site.url }}/assets/images/equation1-4.PNG)

	4. 
	Assignment:  Implement the computation of the windowed Fourier 
	transform of y, given by (3). Evaluate its performance with 
	pure sinusoidal signals and different windows:  
	‚Ä¢ Bartlett 
 	‚Ä¢ Hann 
 	‚Ä¢ Kaiser, 
	
	Compute the spectrogram of an audio track as follows:  
	(a) Decompose a track into a sequence of Nf overlapping 
	frames of size N. The overlap between two frames should be N/2.  
	(b) Compute the magnitude squared of the Fourier transform, 
	|ùëã(ùëò)|2 , k = 1, . . . , K over each frame n.  
	(c) Display the Fourier transform of all the frames in a 
	matrix of size K √ó Nf.  
	
	In order to go between a continuous time perception of sound 
	and a computer‚Äôs discrete time processing, a windowing function 
	is used. This way, we are able to create the desired sound as close 
	as possible without generating high frequency components that could 
	stop us from sampling. Below is the function used to compute the 
	windowed Fourier transform. 
	
	```
	function [Xn] = windows( filename, window ) 
	%windows Implement the computation of the windowed Fourier transform of y 
	frameSize=512; 
	if(nargin == 1)     
		window = 3; 
	end 
	switch window     
		case 1         
			w=bartlett(frameSize); % Bartlett window     
		case 2         
			w=hann(frameSize); % Hann (Hanning) window    
		case 3         
			w=kaiser(frameSize,0.5); % Kaiser window     
		otherwise  
        		w=kaiser(frameSize,0.5); % defaults to Kaiser window 
	end
	[x,~]=extractSound(filename,24); 
	xn=buffer(x,frameSize,frameSize/2); 
	Y=zeros(size(xn)); 
	for i=1:length(xn)     
		Y(:,i)=fft(xn(:,i).*w); 
	end 
	K=frameSize/2+1; 
	Xn=size(Y); 
	for i=1:length(xn)     
		Xn(1:K,i)=Y(1:K,i); 
	end 
	end
	```
	
	Below is the difference in performance between the windows, it seems 
	that the Kaiser window performs the best, but only by about 70 ms.  
	
![Figure11]({{ site.url }}/assets/images/figure1-1.PNG)

	In addition to how quickly each window performs, we can also see how 
	effective each window is at extracting our signal using the spectrogram 
	function, the spectrograms for each window are shown below.  
	
![Figure12]({{ site.url }}/assets/images/figure1-2.PNG)
![Figure13]({{ site.url }}/assets/images/figure1-3.PNG)

	As it can be seen here, the windows are pretty similar and have very 
	similar power to frequency ratios for the ‚Äútrack201-clasical.wav‚Äù track. 
	Below is the spectrogram code. 
	
	
	filename='track201-classical.wav'; 
	[x,~]=audioread(filename); 
	info=audioinfo(filename); 
	nsc = 512; 
	nov = floor(nsc/2); 
	nff = max(256,2^nextpow2(nsc)); 
	figure spectrogram(x,bartlett(nsc),nov,nff,info.SampleRate,'yaxis'); 
	title('Bartlett Window'); 
	ax=gca; 
	saveas(ax,'BartlettWindow.png'); 
	figure spectrogram(x,hamming(nsc),nov,nff,info.SampleRate,'yaxis'); 
	title('Hamming Window'); 
	ax=gca; 
	saveas(ax,'HammingWindow.png'); 
	figure spectrogram(x,kaiser(nsc),nov,nff,info.SampleRate,'yaxis'); 
	title('Kaiser Window'); 
	ax=gca; 
	saveas(ax,'KaiserWindow.png'); 
	close all; 
	
	
	5. Spectral centroid and spread
	For these concepts we will treat the normalized magnitude of a 
	spectral coefficient as if it were the probability that a particular 
	frequency value occurs. In other words, for frame n, we have for the 
	‚Äúprobability‚Äù of frequency k 

![Equation15]({{ site.url }}/assets/images/equation1-5.PNG)

	We then define the spectral centroid (the ‚Äúcenter of mass‚Äù) of the spectrum as 
	
![Equation16]({{ site.url }}/assets/images/equation1-6.PNG)

	where we again think of k as the frequency.
	
	The spectral spread for frame n is then the standard deviation given by 

![Equation17]({{ site.url }}/assets/images/equation1-7.PNG)

	The spectral centroid can be used to quantify sound sharpness or 
	brightness. The spread quantifies the width of the spectrum around 
	the centroid, and thus helps differentiate between tone-like and 
	noiselike sounds. 
	
	6. Spectral Flatness 
	
	Spectral flatness is the ratio between the geometric and arithmetic 
	means of the magnitude of the Fourier transform, 
	
![Equation18]({{ site.url }}/assets/images/equation1-8.PNG)

	The flatness is always smaller than one since the geometric mean is 
	always smaller than the arithmetic mean. The flatness is one, if all 
	|X(k)| are equal. This happens for a very noisy signal. A very small 
	flatness corresponds to the presence of tonal components. In summary, 
	this is a measure of the noisyness of the spectrum. 
	
	7.  Spectral Flux 
	
	The spectral flux is a global measure of the spectral changes between 
	two adjacent frames, n ‚àí 1 and n, 
	
![Equation19]({{ site.url }}/assets/images/equation1-9.PNG)

	where Pn(k) is the normalized frequency distribution for frame n.
	
	8. 
	
	Assignment: Implement all the low level spectral features and evaluate 
	them on the different tracks. Your MATLAB function should display each 
	feature as a time series in separate figure (e.g. see Fig. 3).
	
	Comment on the specificity of the feature, and its ability to separate 
	different musical genres. 
	
	Just as songs have low level time domain characteristics, they also have 
	low level frequency domain characteristics. With modern DSP chips, 
	computing an FFT is no longer that big of an issue for DSP engineers. 
	Below are plots for the frequency centroid, spread, flux, and flatness 
	for the ‚Äútrack201-classical.wav‚Äù track. Plots for all other tracks 
	can be found at the end of the report.
	
![Figure14]({{ site.url }}/assets/images/figure1-4.PNG)
![Figure15]({{ site.url }}/assets/images/figure1-5.PNG)
![Figure16]({{ site.url }}/assets/images/figure1-6.PNG)

	Again, these features are most useful in identifying classical and jazz music. 
	Even though these are more in depth tools, they seem to provide the same amount 
	of usefulness as the time domain analyses. Below is the code used to generate 
	these plots.  
	freqDist.m: 
	
	
	function [ Xk ] = freqDist( filename ) 
	%freqDist Computes the frequency distribution 
	% info=audioinfo(filename); 
	song=extractSound(filename); 
	frames_overlap = buffer(song,512,256); 
	w=kaiser(512); 
	N=512; 
	Y=fft(w.*frames_overlap(:,2)); 
	Xk=Y(1:256); K=N/2+1;   
	for i = 1:length(frames_overlap)     
		Y=fft(w.*frames_overlap(:,i));     
		Xk(1:K,i)=Y(1:K); 
	end 
	end
	
	
	centroidSpread.m:
	
	
	function [ centroid,sigmaK ] = centroidSpread( filename ) 
	%centroid Summary of this function goes here 
	%   The spectral centroid can be used to quantify sound sharpness or 
	%   brightness. The spread quantifies the spread of the spectrum around 
	%   the centroid, and thus helps differentiate between tone-like and 
	%   noise-like sounds. 
	close all; 
	Xk=freqDist(filename); 
	[a,b]=size(Xk); 
	centroid=zeros(1,b); 
	sigmaK=zeros(1,b); 
	for n=1:b     
		for k=1:a         
			centroid(n)=centroid(n)+(k/a*Xk(k,n));     
		end 
	end   
	for n=1:b     
		for k=1:a         
			sigmaK(n)=sigmaK(n)+(1/(a-1))*(Xk(k,n)-centroid(n))^2;     
		end 
	end 
	sigmaK=sqrt(sigmaK); 
	x=1:b;   
	figure plot(x,sigmaK); 
	title(['Frequency Spread: "' filename '"']); 
	xlabel('Frame Number'); ylabel('Spread'); 
	xlim([0,b]); 
	saveas(gca,['freqSpread' filename(1:end-4) '.png']);   
	figure plot(x,centroid); title(['Frequency Centroid: "' filename '"']); 
	xlabel('Frame Number'); 
	ylabel('Centroid'); 
	xlim([0,b]); 
	saveas(gca,['freqCent' filename(1:end-4) '.png']); 
	close all; 
	end 
	
	
	specFlat.m: 
	
	
	function [ SFn ] = specFlat( filename ) 
	%specFlat Computes the spectral flatness of a signal 
	%   Spectral flatness is the ratio between the geometric and arithmetic 
	%   means of the magnitude of the Fourier transform 
	Xk=freqDist(filename); 
	SFn=(geomean(abs(Xk))./mean(abs(Xk)));  
	h=figure; 
	plot(SFn); 
	title(['Spectral Flatness: "' filename '"']); 
	xlabel('Frame Number'); 
	ylabel('Flatness'); 
	xlim([0,length(SFn)]); 
	saveas(gca,['specFlat_' filename(1:end-4) '.png']); 
	close(h); 
	end
	
	
	specFlux.m: 
	
	
	function [ Fn] = specFlux( filename) 
	%specFlux Measures how quickly power spectrum is changing 
	%   The spectral flux is a global measure of the spectral changes between 
	%   two adjacent frames, n-1 and n 
	Xk=freqDist(filename); 
	Fn=sum(diff(Xk).^2);   
	close all; 
	figure plot(Fn); 
	title(['Spectral Flux: "' filename '"']); 
	xlabel('Frame Number'); 
	ylabel('Flux'); 
	xlim([0,length(Fn)]); 
	saveas(gca,['specFlux' filename(1:end-4) '.png']); 
	close all; 
	end
	
	
	9. Application: Mpeg7 Low Level Audio Descriptors 
	MPEG 7, also known as ‚ÄúMultimedia Content Description Interface,‚Äù 
	provides a standardized set of technologies for describing 
	multimedia content. Part 4 of the standard specifies description 
	tools that pertain to multimedia in the audio domain. The standard 
	defines Low-level Audio Descriptors (LLDs) that consist of a 
	collection of simple, low complexity descriptors to characterize 
	the audio content. Some of the features are similar to the 
	spectral features defined above.
	
2. **Basic Psychoacoustic Quantities**

In order to develop more sophisticated algorithms to analyze music based on its content, we need to define several subjective features such as timbre, melody, harmony, rhythm, tempo, mood, lyrics, etc. Some of these concepts can be defined formally, while others are more subjective and can be formalized using a wide variety of different algorithms. We will focus on the features that can be defined mathematically. 

	1. Psychoacoustic 
	
	Psychoacoustics involves the study of the human auditory system, 
	and the formal quantification of the relationships between the 
	physics of sounds, and our perception of audio. We will 
	describe some key aspects of the human auditory system:  
	
	the perception of frequencies and pitch for pure and 
	complex tones;
	
	the frequency selectivity of the auditory system:
	our ability to perceive two similar frequencies as 
	distinct; 
	
	the modeling of the auditory system as a bank 
	of auditory filters;  

	the perception of loudness;
	
	the perception of rhythm. 
	
	2. Perception of frequencies 
	
	The auditory system, like the visual system, is able 
	to detect frequencies over a wide range of scales. 
	In order to measure frequencies over a very large 
	range, it operates using a logarithmic scale. Let 
	us consider a pure tone, modeled by a sinusoidal 
	signal oscillating at a frequency f. If f < 500 Hz, 
	then the perceived tone ‚Äì or pitch ‚Äì varies as a 
	linear function of f. When f > 1, 000 Hz, then 
	the perceived pitch increases logarithmically 
	with f. Several frequency scales have been 
	proposed to capture the logarithmic scaling 
	of frequency perception. 
	
	3. The mel/Bark scale
	
	The Bark scale (named after the German physicist 
	Barkhausen) is defined as 

![Equation110]({{ site.url }}/assets/images/equation1-10.PNG)

	where f is measured in Hz. The mel-scale is defined 
	by the fact that 1 bark = 100 mel. In this lab 
	we will use a slightly modified version of 
	the mel scale defined by 
	
![Equation111]({{ site.url }}/assets/images/equation1-11.PNG)
	
	Note that, by construction, the m value corresponding to 
	1000 Hz is 1000 (the logarithm in the above formula is 
	the natural log) 
	
	4. The cochlear filterbank
	
	Finally, we need to account for the fact that the 
	auditory system behaves as a set of filters, 
	i.e. as a filterbank, whose filters have overlapping 
	frequency responses. For each filter, the range of 
	frequencies over which the filter response is 
	significant is called a critical band. A 
	critical band is a band of audio frequencies 
	within which a second tone will interfere with 
	the first via auditory masking.  Our perception 
	of pitch can be quantified using the total energy 
	at the output of each filter. All spectral energy 
	that falls into one critical band is summed up, 
	leading to a single number for that band.  
	We describe in the following a simple model of 
	the cochlear filterbank. The filter bank is constructed 
	using NB = 40 logarithmically spaced triangle filters 
	centered at the frequencies ‚Ñ¶p, which are implicitly 
	defined by 
	
![Equation112]({{ site.url }}/assets/images/equation1-12.PNG)
	
	The sequence of mel frequencies corresponding to the 
	‚Ñ¶p are chosen to be equally spaced in the mel scale. 
	Letting the indexing of the center frequencies of 
	the filters start with p = 1 we have 
	
![Equation113]({{ site.url }}/assets/images/equation1-13.PNG)

	where NB is the number of filters in the filterbank, 
	
![Equation114]({{ site.url }}/assets/images/equation1-14.PNG)

	and fs is the sampling frequency of the audio being analyzed. 
	As mentioned above, we‚Äôll let NB = 40. We see that melmax 
	is the m value corresponding to half the sampling frequency, 
	which is the highest frequency preserved in the digital 
	signal. We also see that melmin corresponds to a frequency 
	of 20 Hz. (These min and max values are not used as center 
	frequencies for any filters, rather, they are the upper 
	and lower frequency limits that are covered by the 
	filters in the filter bank.) 
	
	Each filter Hp is centered around the frequency ‚Ñ¶p and is 
	defined by 
	
![Equation115]({{ site.url }}/assets/images/equation1-15.PNG)

	Each triangular filter is normalized such that the integral of 
	each filter is 1. In addition, the filters overlap so that the 
	frequency at which the filter Hp is maximum is the starting 
	frequency for the next filter Hp+1, and the edge frequency 
	of Hp-1. Finally, the mel-spectrum (MFCC) coefficients of 
	the n-th frame are defined for p = 1, ‚Ä¶, NB as
	
![Equation116]({{ site.url }}/assets/images/equation1-16.PNG)

	where the filter Hp is a discrete implementation of 
	the continuous filter defined by (17). The discrete filter 
	is normalized such that 
	
![Equation117]({{ site.url }}/assets/images/equation1-17.PNG)

	5. Assigment
	
	Implement the computation of the triangular filterbanks 
	Hp, p = 1, . . . , NB. Your function will return an array 
	fbank of size NB √ó K such that fbank(p,:) contains 
	the filter bank Hp. 
	
	Implement the computation of the mfcc coefficients, 
	as defined in (18).
	
	Below is the code used to create the filter bank, 
	and are now one step closer to defining music by 
	genre. Now we can categorize pitches more closely 
	to how a human ear would do so. 
	
	function [ fbank ] = melBank(~) 
	%melBank Creates a set of mel filter banks 
	%   Implement the computation of the triangular filterbanks 
	%   Hp, p = 1,...,NB. Your function will return an array fbank of size 
	%   NB x K such that fbank(p,:) contains the filter bank Hp.   
	nbanks = 40; % Number of Mel frequency bands 
	% linear frequencies 
	N=512; 
	K=N/2+1; 
	% fbank=zeros(nbanks,K); 
	fs=11025; linFrq = 20:fs/2; 
	% mel frequencies 
	melFrq = log ( 1 + linFrq/700) *1127.01048; 
	% equispaced mel indices 
	melIdx = linspace(1,max(melFrq),nbanks+2); 
	% From mel index to linear frequency 
	melIdx2Frq = zeros (1,nbanks+2); 
	% melIdx2Frq (p) = \Omega_p 
	for i=1:nbanks+2     
		[~, indx] = min(abs(melFrq - melIdx(i)));     
		melIdx2Frq(i) = linFrq(indx); 
	end   
	% map frequency banks 
	fbank_freq = linspace(0,floor(fs/2),K);   
	fbank=zeros(nbanks,K);
	% Implement equation for mel filters 
	for i = 2:nbanks+1     
		range=2/(melIdx2Frq(i+1)-melIdx2Frq(i-1));     
		for j=1:K         
			filt_val=range;         
			if(fbank_freq(j) <= melIdx2Frq(i)) ...                 
				&& (fbank_freq(j) > melIdx2Frq(i-1))            
			filt_val = filt_val*((fbank_freq(j) -melIdx2Frq(i-1)) ...                 
				/(melIdx2Frq(i) - melIdx2Frq(i-1)));             
			fbank(i-1,j) = filt_val; 
			elseif(fbank_freq(j) < melIdx2Frq(i+1)) ...                 
					&& (fbank_freq(j) >= melIdx2Frq(i))             
				filt_val = filt_val*((melIdx2Frq(i+1) - fbank_freq(j))...                 
					/(melIdx2Frq(i+1) - melIdx2Frq(i)));             
				fbank(i-1,j) = filt_val;         
			else             
				fbank(i-1,j)=0;         
			end     
		end 
	end   
	for i=1:nbanks     
		norm=sum(fbank(i,:));     
		for j=1:K         
			fbank(i,j)=fbank(i,j)/norm;     
		end 
	end   
	if(nargin)     
		close all;     
		figure     
		plot(fbank.');     
		title('Mel Filter Bank');     
		xlabel('Frequency (Hz)');     
		ylabel('Filter Magnitude');     
		xlim([0,length(fbank)]);     
		saveas(gca,'melFilterBank.png');     
		close all; 
	end 
	end
	
	Here is the code used to calculate the mfcc coefficients: 
	
	function [ mfccp ] = mfcc( fbank,Xn ) 
	%mfcc Computes the Mel frequency coeffecients 
	%   The mel-spectrum (MFCC) coefficient of the n-th frame is defined for 
	%   p = 1,...,NB 
	% fbank=fbank/max(fbank); 
	[a,~]=size(fbank); 
	[c,d]=size(Xn); 
	% a should be 40 
	% c should be 257 
	% d should be num frames (1032) 
	mfccp=zeros(a,d); 
	for n=1:d % frames     
		mfcc_val=0;     
		for p=1:a % banks         
			for k=1:c             
				mfcc_val = mfcc_val + abs((fbank(p,k)*Xn(k,n))^2);        
			end         
			mfccp(p,n)=mfcc_val;     
		end 
	end 
	mfccp=10*log(mfccp)/log(10); 
	end 
	
	Here is a plot of the Mel Filter Bank: 
	
![Figure17]({{ site.url }}/assets/images/figure1-7.PNG)
	

	
	
	
	


 
